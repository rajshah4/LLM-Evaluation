{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG QA Arena Benchmark: Complete Evaluation Pipeline\n",
    "\n",
    "This notebook provides a comprehensive pipeline for evaluating Retrieval-Augmented Generation (RAG) systems using the [RAG QA Arena benchmark](https://github.com/awslabs/rag-qa-arena).\n",
    "\n",
    "## What The Notebook Does\n",
    "- How to ingest 30GB+ document collections into Contextual AI Datastore\n",
    "- Performance optimization for large-scale document processing  \n",
    "- Retrieval evaluation metrics and answer equivalence testing\n",
    "- Production-ready error handling and recovery procedures\n",
    "\n",
    "**Datasets**: LOTTE (~150K docs) and FIQA (~57K docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We import all necessary libraries for data handling, evaluation, and asynchronous operations.  \n",
    "The notebook uses `pandas` for dataframes, `numpy` for numerical operations, and `ragas` for retrieval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Async operations and file handling\n",
    "import asyncio\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# RAG evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_recall, context_precision, faithfulness\n",
    "\n",
    "# Contextual AI client\n",
    "from contextual import ContextualAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Dataset Overview\n",
    "\n",
    "| Dataset | Size | Documents | Domain |\n",
    "|---------|------|-----------|--------|\n",
    "| LOTTE | 30GB | 150K+ | Lifestyle/Recreation |\n",
    "| FIQA | - | 57K | Financial QA |\n",
    "\n",
    "### Download Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To replicate the RAG QA Benchmark we need two datasets for the corpus, LOTTE and FIQA, and the annotoated question/answers which are available at https://github.com/awslabs/rag-qa-arena/tree/main/data\n",
    "\n",
    "The bulk of this notebook focuses on showing how to load LOTTE and FIQA into a Contextual AI Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Lotte Passages\n",
    "\n",
    "Lotte is about 30GBs so be prepared\n",
    "\n",
    "I did this:\n",
    "```\n",
    "# Clone the repo\n",
    "git clone https://huggingface.co/datasets/colbertv2/lotte_passages\n",
    "\n",
    "# Navigate into the repo\n",
    "cd lotte_passages\n",
    "\n",
    "# Pull the large files\n",
    "git lfs pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 /Users/rajivshah/Code/lotte_passages/recreation/dev_collection.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking up a random line\n",
    "!sed -n '72610p' /Users/rajivshah/Code/lotte_passages/recreation/test_collection.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to check the number of lines in the file\n",
    "!wc -l /Users/rajivshah/Code/lotte_passages/recreation/test_collection.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download FIQA\n",
    "\n",
    "You can get FIQA from https://huggingface.co/datasets/BeIR/fiqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"BeIR/fiqa\", \"corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['corpus'][42]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Annotated Questions and Answers:\n",
    "\n",
    "From: https://github.com/awslabs/rag-qa-arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rajistics/rag-qa-arena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out Lotte recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreation_data = dataset['recreation']\n",
    "print(recreation_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 2 /Users/rajivshah/Code/rag-qa-arena/data/annotations_recreation_with_citation.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '72610p' /Users/rajivshah/Code/lotte_passages/recreation/test_collection.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tail -n 2 /Users/rajivshah/Code/rag-qa-arena/data/annotations_recreation_with_citation.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '152852p' /Users/rajivshah/Code/lotte_passages/recreation/test_collection.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out FIQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 2 /Users/rajivshah/Code/rag-qa-arena/data/annotations_fiqa_with_citation.jsonl\n",
    "fiqa_data = dataset['fiqa']\n",
    "print(fiqa_data[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['corpus'][42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ds['corpus'].filter(lambda x: x['_id'] == '382384')\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest into Contextual AI Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextual import ContextualAI\n",
    "client = ContextualAI()  ## use your API key if you already set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = client.datastores.create(name=\"RAGQA_Recreation2\")\n",
    "datastore_id = result.id\n",
    "print(f\"Datastore ID: {datastore_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSONL Document Ingestion Script Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This script processes large JSONL (JSON Lines) files containing document collections and uploads them to a datastore while maintaining precise tracking of document relationships. It's specifically designed for handling large document collections (like the 150K+ document LOTTE passages collection) with controlled concurrency to prevent system overload.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### Document Processing\n",
    "- **JSONL Parsing**: Reads files where each line contains a JSON document with fields like `doc_id`, `author`, and `text`\n",
    "- **Markdown Formatting**: Converts each document to text with metadata headers and content sections\n",
    "- **Meaningful Filenames**: Creates temporary files named `doc_{document_id}.md` instead of random temporary names\n",
    "\n",
    "### Concurrency Control\n",
    "- **Throttled Uploads**: Limits concurrent processing to prevent overwhelming the datastore (default: 4 simultaneous documents)\n",
    "- **Smart Waiting**: Monitors document status and waits for available processing slots before uploading new documents\n",
    "- **Status Monitoring**: Regularly checks and reports on document processing states (pending, processing, processed)\n",
    "\n",
    "### Comprehensive Tracking\n",
    "- **Line Number Mapping**: Maintains exact relationship between original JSONL line numbers and datastore document IDs\n",
    "- **CSV Logging**: Creates persistent `document_mapping.csv` file with complete upload history\n",
    "- **Progress Monitoring**: Shows real-time processing rates and status breakdowns\n",
    "\n",
    "## How It Works\n",
    "\n",
    "### 1. Document Reading and Parsing\n",
    "```\n",
    "JSONL File (line 152852) â†’ Parse JSON â†’ Extract doc_id, author, text\n",
    "```\n",
    "\n",
    "### 2. Markdown Conversion\n",
    "Each document is formatted as:\n",
    "```markdown\n",
    "# Document {doc_id}\n",
    "\n",
    "**Author:** {author}\n",
    "**Original Doc ID:** {doc_id}\n",
    "**Line Number:** {line_number}\n",
    "\n",
    "---\n",
    "\n",
    "## Content\n",
    "\n",
    "{formatted_text}\n",
    "```\n",
    "\n",
    "### 3. Concurrency Management\n",
    "- Before each upload, checks current processing queue\n",
    "- If queue is full (â‰¥4 documents), waits and checks every 30 seconds\n",
    "- Only proceeds when a processing slot becomes available\n",
    "\n",
    "### 4. Upload and Tracking\n",
    "- Uploads document to datastore via API\n",
    "- Records mapping: `line_number â†’ original_doc_id â†’ datastore_document_id`\n",
    "- Logs to CSV file for permanent tracking\n",
    "- Displays progress statistics\n",
    "\n",
    "### 5. Status Monitoring\n",
    "- Shows processing rates (documents/minute)\n",
    "- Provides status breakdowns every few uploads\n",
    "- Final status summary at completion\n",
    "\n",
    "## Output Files\n",
    "\n",
    "### document_mapping.csv\n",
    "Permanent tracking file with columns:\n",
    "- `line_number`: Original position in JSONL file\n",
    "- `original_doc_id`: Document ID from JSON data\n",
    "- `datastore_document_id`: New ID assigned by datastore\n",
    "- `author`: Document author\n",
    "- `text_preview`: First 100 characters for identification\n",
    "\n",
    "### Temporary Files\n",
    "- Created as `doc_{document_id}.txt` in system temp directory\n",
    "- Automatically cleaned up after upload\n",
    "- Visible in datastore interface during processing\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "- `max_documents`: Limit processing to first N documents (useful for testing)\n",
    "- `max_concurrent`: Maximum simultaneous processing documents (default: 4)\n",
    "- `check_interval`: How often to check status when waiting (default: 30 seconds)\n",
    "- `mapping_file`: Name of CSV tracking file (default: \"document_mapping.csv\")\n",
    "\n",
    "## Usage Patterns\n",
    "\n",
    "### Testing (10 documents)\n",
    "```python\n",
    "mappings = ingest_documents_with_tracking(\n",
    "    jsonl_file_path=\"test_collection.jsonl\",\n",
    "    client=client,\n",
    "    datastore_id=datastore_id,\n",
    "    max_documents=10\n",
    ")\n",
    "```\n",
    "\n",
    "### Production (with rate limiting)\n",
    "```python\n",
    "mappings = ingest_documents_with_tracking(\n",
    "    jsonl_file_path=\"test_collection.jsonl\",\n",
    "    client=client,\n",
    "    datastore_id=datastore_id,\n",
    "    max_documents=1000,\n",
    "    max_concurrent=4,\n",
    "    check_interval=30\n",
    ")\n",
    "```\n",
    "\n",
    "## Utility Functions\n",
    "\n",
    "- `load_mapping_from_file()`: Read existing mapping data from CSV\n",
    "- `find_document_by_line_number()`: Look up document info by original line number\n",
    "- `get_active_document_counts()`: Check current processing queue status\n",
    "- `analyze_document_status()`: Display comprehensive status breakdown\n",
    "\n",
    "## Benefits\n",
    "\n",
    "1. **Traceability**: Always know which datastore document corresponds to which original line\n",
    "2. **Reliability**: Handles failures gracefully, continues processing remaining documents\n",
    "3. **Efficiency**: Respects system limits while maintaining good throughput\n",
    "4. **Resumability**: CSV tracking allows resuming from failures or adding more documents later\n",
    "5. **Visibility**: Clear progress reporting and status monitoring throughout the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def get_active_document_counts(client, datastore_id: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Get count of documents currently being processed or pending.\n",
    "    \n",
    "    Args:\n",
    "        client: API client instance\n",
    "        datastore_id: ID of the datastore\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (processing count, pending count)\n",
    "    \"\"\"\n",
    "    response = client.datastores.documents.list(datastore_id)\n",
    "    processing = 0\n",
    "    pending = 0\n",
    "    \n",
    "    for doc in response.documents:\n",
    "        if doc.status == 'processing':\n",
    "            processing += 1\n",
    "        elif doc.status == 'pending':\n",
    "            pending += 1\n",
    "            \n",
    "    return processing, pending\n",
    "\n",
    "def wait_for_available_slot(client, datastore_id: str, max_concurrent: int = 4, \n",
    "                        check_interval: int = 30) -> None:\n",
    "    \"\"\"\n",
    "    Wait until there's a free slot for processing or pending.\n",
    "    \n",
    "    Args:\n",
    "        client: API client instance\n",
    "        datastore_id: ID of the datastore\n",
    "        max_concurrent: Maximum number of concurrent documents (processing or pending)\n",
    "        check_interval: How often to check status in seconds\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        processing_count, pending_count = get_active_document_counts(client, datastore_id)\n",
    "        total_active = processing_count + pending_count\n",
    "        \n",
    "        if total_active < max_concurrent:\n",
    "            return\n",
    "            \n",
    "        print(f\"Currently {processing_count} processing and {pending_count} pending documents.\")\n",
    "        print(f\"Waiting {check_interval} seconds for a slot to become available...\")\n",
    "        time.sleep(check_interval)\n",
    "\n",
    "def analyze_document_status(client, datastore_id: str):\n",
    "    \"\"\"\n",
    "    Analyze the status of documents in the datastore\n",
    "    \n",
    "    Args:\n",
    "        client: API client instance\n",
    "        datastore_id: ID of the datastore\n",
    "    \"\"\"\n",
    "    response = client.datastores.documents.list(datastore_id)\n",
    "    status_counts = {}\n",
    "    \n",
    "    for doc in response.documents:\n",
    "        status = doc.status\n",
    "        status_counts[status] = status_counts.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"\\nDocument Status Analysis\")\n",
    "    print(f\"----------------------\")\n",
    "    print(f\"Total Documents in System: {len(response.documents)}\")\n",
    "    print(\"\\nStatus Breakdown:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"- {status}: {count} documents\")\n",
    "\n",
    "def ingest_documents_with_tracking(\n",
    "    jsonl_file_path: str, \n",
    "    client, \n",
    "    datastore_id: str, \n",
    "    max_documents: int = 10,\n",
    "    max_concurrent: int = 4,\n",
    "    check_interval: int = 30,\n",
    "    mapping_file: str = \"document_mapping.csv\"\n",
    ") -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"\n",
    "    Ingest documents from JSONL file and track line number to document ID mapping.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_file_path: Path to the JSONL file\n",
    "        client: Your datastore client\n",
    "        datastore_id: Target datastore ID\n",
    "        max_documents: Maximum number of documents to process\n",
    "        max_concurrent: Maximum number of documents processing/pending at once\n",
    "        check_interval: How often to check status when waiting (seconds)\n",
    "        mapping_file: CSV file to store the mapping\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (line_number, doc_id, document_id)\n",
    "    \"\"\"\n",
    "    \n",
    "    mappings = []\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize or append to mapping file\n",
    "    file_exists = os.path.exists(mapping_file)\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        with open(mapping_file, 'a', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Write header if file is new\n",
    "            if not file_exists:\n",
    "                writer.writerow(['line_number', 'original_doc_id', 'datastore_document_id', 'author', 'text_preview'])\n",
    "            \n",
    "            for line_number, line in enumerate(jsonl_file, 1):\n",
    "                if processed_count >= max_documents:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    # Parse the JSON line\n",
    "                    doc_data = json.loads(line.strip())\n",
    "                    \n",
    "                    # Extract document information\n",
    "                    original_doc_id = doc_data.get('doc_id', 'unknown')\n",
    "                    author = doc_data.get('author', 'unknown')\n",
    "                    text = doc_data.get('text', '')\n",
    "                    text_preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "                    \n",
    "                    # Wait for an available slot before processing\n",
    "                    print(f\"\\nPreparing to process line {line_number} (Doc ID: {original_doc_id})\")\n",
    "                    wait_for_available_slot(client, datastore_id, max_concurrent, check_interval)\n",
    "                    \n",
    "                    # Create a temporary markdown file with the document content using the document ID\n",
    "                    # Sanitize the document ID for filename use\n",
    "                    safe_doc_id = str(original_doc_id).replace('/', '_').replace('\\\\', '_')\n",
    "                    temp_filename = f\"doc_{safe_doc_id}.txt\"\n",
    "                    temp_file_path = os.path.join(tempfile.gettempdir(), temp_filename)\n",
    "                    \n",
    "                    with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                        # Write document metadata and content in markdown format\n",
    "                        temp_file.write(f\"# Document {original_doc_id}\\n\\n\")\n",
    "                        #temp_file.write(f\"**Author:** {author}  \\n\")\n",
    "                        temp_file.write(f\"**Original Doc ID:** {original_doc_id}  \\n\")\n",
    "                        temp_file.write(f\"**Line Number:** {line_number}  \\n\\n\")\n",
    "                        temp_file.write(\"---\\n\\n\")\n",
    "                        temp_file.write(f\"## Content\\n\\n\")\n",
    "                        \n",
    "                        # Format the main content - you could add paragraph breaks or other formatting here\n",
    "                        formatted_text = text.replace('\\n\\n', '\\n\\n')  # Ensure proper paragraph spacing\n",
    "                        temp_file.write(formatted_text)\n",
    "                        # temp_file_path is already set above\n",
    "                    \n",
    "                    try:\n",
    "                        # Ingest the document\n",
    "                        with open(temp_file_path, 'rb') as f:\n",
    "                            ingestion_result = client.datastores.documents.ingest(datastore_id, file=f)\n",
    "                            datastore_document_id = ingestion_result.id\n",
    "                        \n",
    "                        # Record the mapping\n",
    "                        mapping_entry = (line_number, original_doc_id, datastore_document_id)\n",
    "                        mappings.append(mapping_entry)\n",
    "                        \n",
    "                        # Write to CSV\n",
    "                        writer.writerow([line_number, original_doc_id, datastore_document_id, author, text_preview])\n",
    "                        \n",
    "                        processed_count += 1\n",
    "                        elapsed_time = (time.time() - start_time) / 60  # in minutes\n",
    "                        rate = processed_count / elapsed_time if elapsed_time > 0 else 0\n",
    "                        \n",
    "                        print(f\"âœ“ Line {line_number}: Original ID {original_doc_id} -> Datastore ID {datastore_document_id}\")\n",
    "                        print(f\"   Processing rate: {rate:.1f} documents/minute\")\n",
    "                        \n",
    "                        # Show current status every few documents\n",
    "                        if processed_count % 3 == 0:\n",
    "                            analyze_document_status(client, datastore_id)\n",
    "                        \n",
    "                    finally:\n",
    "                        # Clean up temporary file\n",
    "                        os.unlink(temp_file_path)\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"âœ— Line {line_number}: JSON parsing error - {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Line {line_number}: Ingestion error - {e}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"\\nProcessed {processed_count} documents successfully\")\n",
    "    print(f\"Mapping saved to: {mapping_file}\")\n",
    "    \n",
    "    # Final status check\n",
    "    print(f\"\\nFinal status check:\")\n",
    "    analyze_document_status(client, datastore_id)\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "def load_mapping_from_file(mapping_file: str = \"document_mapping.csv\") -> Dict[int, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load the line number to document ID mapping from CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping line_number to document info\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    if not os.path.exists(mapping_file):\n",
    "        print(f\"Mapping file {mapping_file} not found\")\n",
    "        return mapping\n",
    "    \n",
    "    with open(mapping_file, 'r', newline='') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            line_num = int(row['line_number'])\n",
    "            mapping[line_num] = {\n",
    "                'original_doc_id': row['original_doc_id'],\n",
    "                'datastore_document_id': row['datastore_document_id'],\n",
    "                'author': row['author'],\n",
    "                'text_preview': row['text_preview']\n",
    "            }\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def find_document_by_line_number(line_number: int, mapping_file: str = \"document_mapping.csv\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Find document information by line number.\n",
    "    \"\"\"\n",
    "    mapping = load_mapping_from_file(mapping_file)\n",
    "    return mapping.get(line_number, {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your actual values\n",
    "JSONL_FILE_PATH = \"/Users/rajivshah/Code/lotte_passages/recreation/test_collection.jsonl\"\n",
    "# CLIENT = your_client_instance\n",
    "# DATASTORE_ID = \"your_datastore_id\"\n",
    "\n",
    "print(\"Document Ingestion Test Script\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "mappings = ingest_documents_with_tracking(\n",
    "    jsonl_file_path=JSONL_FILE_PATH,\n",
    "    client=client,\n",
    "    datastore_id=datastore_id,\n",
    "    max_documents=100,\n",
    "    max_concurrent=10,  # Only allow 4 documents processing/pending at once\n",
    "    check_interval=20  # Check every 30 seconds when waiting\n",
    ")\n",
    "\n",
    "# For now, just show what the mapping file would look like\n",
    "print(\"This script will create a CSV mapping file with columns:\")\n",
    "print(\"- line_number: Original line number from JSONL file\")\n",
    "print(\"- original_doc_id: The 'doc_id' field from the JSON\")\n",
    "print(\"- datastore_document_id: The ID returned by the datastore\")\n",
    "print(\"- author: Document author\")\n",
    "print(\"- text_preview: First 100 characters of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest FIQA a Hugging Face dataset\n",
    "\n",
    "This script is modified from above for a Hugging Face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_active_document_counts(client, datastore_id: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Get count of documents currently being processed or pending.\n",
    "    \n",
    "    Args:\n",
    "        client: API client instance\n",
    "        datastore_id: ID of the datastore\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (processing count, pending count)\n",
    "    \"\"\"\n",
    "    response = client.datastores.documents.list(datastore_id)\n",
    "    processing = 0\n",
    "    pending = 0\n",
    "    \n",
    "    for doc in response.documents:\n",
    "        if doc.status == 'processing':\n",
    "            processing += 1\n",
    "        elif doc.status == 'pending':\n",
    "            pending += 1\n",
    "            \n",
    "    return processing, pending\n",
    "\n",
    "def wait_for_available_slot(client, datastore_id: str, max_concurrent: int = 4, \n",
    "                        check_interval: int = 30) -> None:\n",
    "    \"\"\"\n",
    "    Wait until there's a free slot for processing or pending.\n",
    "    \n",
    "    Args:\n",
    "        client: API client instance\n",
    "        datastore_id: ID of the datastore\n",
    "        max_concurrent: Maximum number of concurrent documents (processing or pending)\n",
    "        check_interval: How often to check status in seconds\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        processing_count, pending_count = get_active_document_counts(client, datastore_id)\n",
    "        total_active = processing_count + pending_count\n",
    "        \n",
    "        if total_active < max_concurrent:\n",
    "            return\n",
    "            \n",
    "        print(f\"Currently {processing_count} processing and {pending_count} pending documents.\")\n",
    "        print(f\"Waiting {check_interval} seconds for a slot to become available...\")\n",
    "        time.sleep(check_interval)\n",
    "\n",
    "def analyze_document_status(client, datastore_id: str):\n",
    "    \"\"\"\n",
    "    Analyze the status of documents in the datastore\n",
    "    \n",
    "    Args:\n",
    "        client: API client instance\n",
    "        datastore_id: ID of the datastore\n",
    "    \"\"\"\n",
    "    response = client.datastores.documents.list(datastore_id)\n",
    "    status_counts = {}\n",
    "    \n",
    "    for doc in response.documents:\n",
    "        status = doc.status\n",
    "        status_counts[status] = status_counts.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"\\nDocument Status Analysis\")\n",
    "    print(f\"----------------------\")\n",
    "    print(f\"Total Documents in System: {len(response.documents)}\")\n",
    "    print(\"\\nStatus Breakdown:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"- {status}: {count} documents\")\n",
    "\n",
    "def ingest_hf_dataset_with_tracking(\n",
    "    dataset,\n",
    "    client, \n",
    "    datastore_id: str, \n",
    "    max_documents: int = 10,\n",
    "    max_concurrent: int = 4,\n",
    "    check_interval: int = 30,\n",
    "    mapping_file: str = \"hf_document_mapping.csv\"\n",
    ") -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"\n",
    "    Ingest documents from an already-loaded Hugging Face dataset and track row index to document ID mapping.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Already loaded Hugging Face dataset (e.g., ds['corpus'])\n",
    "        client: Your datastore client\n",
    "        datastore_id: Target datastore ID\n",
    "        max_documents: Maximum number of documents to process\n",
    "        max_concurrent: Maximum number of documents processing/pending at once\n",
    "        check_interval: How often to check status when waiting (seconds)\n",
    "        mapping_file: CSV file to store the mapping\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (row_index, original_id, datastore_document_id)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Using dataset with {len(dataset)} documents\")\n",
    "    \n",
    "    mappings = []\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize or append to mapping file\n",
    "    file_exists = os.path.exists(mapping_file)\n",
    "    \n",
    "    with open(mapping_file, 'a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header if file is new\n",
    "        if not file_exists:\n",
    "            writer.writerow(['row_index', 'original_id', 'datastore_document_id', 'title', 'text_preview'])\n",
    "        \n",
    "        for row_index, row in enumerate(dataset):\n",
    "            if processed_count >= max_documents:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Extract document information\n",
    "                original_id = row.get('_id', f'row_{row_index}')\n",
    "                title = row.get('title', '')\n",
    "                text = row.get('text', '')\n",
    "                text_preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "                \n",
    "                # Wait for an available slot before processing\n",
    "                print(f\"\\nPreparing to process row {row_index} (ID: {original_id})\")\n",
    "                wait_for_available_slot(client, datastore_id, max_concurrent, check_interval)\n",
    "                \n",
    "                # Create a temporary text file with the document content using the document ID\n",
    "                # Sanitize the document ID for filename use\n",
    "                safe_doc_id = str(original_id).replace('/', '_').replace('\\\\', '_')\n",
    "                temp_filename = f\"doc_{safe_doc_id}.txt\"\n",
    "                temp_file_path = os.path.join(tempfile.gettempdir(), temp_filename)\n",
    "                \n",
    "                with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                    # Write document metadata and content\n",
    "                    temp_file.write(f\"Original ID: {original_id}\\n\")\n",
    "                    temp_file.write(f\"Title: {title}\\n\")\n",
    "                    temp_file.write(f\"Row Index: {row_index}\\n\")\n",
    "                    temp_file.write(\"---\\n\")\n",
    "                    temp_file.write(text)\n",
    "                \n",
    "                try:\n",
    "                    # Ingest the document\n",
    "                    with open(temp_file_path, 'rb') as f:\n",
    "                        ingestion_result = client.datastores.documents.ingest(datastore_id, file=f)\n",
    "                        datastore_document_id = ingestion_result.id\n",
    "                    \n",
    "                    # Record the mapping\n",
    "                    mapping_entry = (row_index, original_id, datastore_document_id)\n",
    "                    mappings.append(mapping_entry)\n",
    "                    \n",
    "                    # Write to CSV\n",
    "                    writer.writerow([row_index, original_id, datastore_document_id, title, text_preview])\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                    elapsed_time = (time.time() - start_time) / 60  # in minutes\n",
    "                    rate = processed_count / elapsed_time if elapsed_time > 0 else 0\n",
    "                    \n",
    "                    print(f\"âœ“ Row {row_index}: Original ID {original_id} -> Datastore ID {datastore_document_id}\")\n",
    "                    print(f\"   Processing rate: {rate:.1f} documents/minute\")\n",
    "                    \n",
    "                    # Show current status every few documents\n",
    "                    if processed_count % 3 == 0:\n",
    "                        analyze_document_status(client, datastore_id)\n",
    "                    \n",
    "                finally:\n",
    "                    # Clean up temporary file\n",
    "                    if os.path.exists(temp_file_path):\n",
    "                        os.unlink(temp_file_path)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Row {row_index}: Processing error - {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nProcessed {processed_count} documents successfully\")\n",
    "    print(f\"Mapping saved to: {mapping_file}\")\n",
    "    \n",
    "    # Final status check\n",
    "    print(f\"\\nFinal status check:\")\n",
    "    analyze_document_status(client, datastore_id)\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "def load_hf_mapping_from_file(mapping_file: str = \"hf_document_mapping.csv\") -> Dict[int, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load the row index to document ID mapping from CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping row_index to document info\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    if not os.path.exists(mapping_file):\n",
    "        print(f\"Mapping file {mapping_file} not found\")\n",
    "        return mapping\n",
    "    \n",
    "    with open(mapping_file, 'r', newline='') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            row_idx = int(row['row_index'])\n",
    "            mapping[row_idx] = {\n",
    "                'original_id': row['original_id'],\n",
    "                'datastore_document_id': row['datastore_document_id'],\n",
    "                'title': row['title'],\n",
    "                'text_preview': row['text_preview']\n",
    "            }\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def find_document_by_row_index(row_index: int, mapping_file: str = \"hf_document_mapping.csv\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Find document information by row index.\n",
    "    \"\"\"\n",
    "    mapping = load_hf_mapping_from_file(mapping_file)\n",
    "    return mapping.get(row_index, {})\n",
    "\n",
    "def find_document_by_original_id(original_id: str, mapping_file: str = \"hf_document_mapping.csv\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Find document information by original _id field.\n",
    "    \"\"\"\n",
    "    mapping = load_hf_mapping_from_file(mapping_file)\n",
    "    for row_idx, doc_info in mapping.items():\n",
    "        if doc_info['original_id'] == original_id:\n",
    "            return {**doc_info, 'row_index': row_idx}\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextual import ContextualAI\n",
    "client = ContextualAI(api_key=\"key-bl-VPQtBsnkL2v0Gp0g9UZth2xs6tLl2Q06QQwuTKPuzyJRu8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already run:\n",
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"BeIR/fiqa\", \"corpus\")\n",
    "\n",
    "# CLIENT = your_client_instance\n",
    "DATASTORE_ID = \"7363c35f-8177-4182-b4f8-0daf6224251f\"\n",
    "\n",
    "print(\"Hugging Face Dataset Ingestion Script\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Uncomment and modify these lines when you have your client setup:\n",
    "\n",
    "# Use the already-loaded dataset\n",
    "mappings = ingest_hf_dataset_with_tracking(\n",
    "    dataset=ds['corpus'],  # or just ds if it's not a DatasetDict\n",
    "    client=client,\n",
    "    datastore_id=DATASTORE_ID,\n",
    "    max_documents=60000,\n",
    "    max_concurrent=50,\n",
    "    check_interval=20\n",
    ")\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nTesting mapping retrieval:\")\n",
    "# By row index\n",
    "doc_info = find_document_by_row_index(42)\n",
    "if doc_info:\n",
    "    print(f\"Row 42: {doc_info}\")\n",
    "\n",
    "# By original ID\n",
    "doc_info = find_document_by_original_id('382384')\n",
    "if doc_info:\n",
    "    print(f\"ID 382384: {doc_info}\")\n",
    "\n",
    "\n",
    "# For now, just show what the mapping file would look like\n",
    "print(\"This script will create a CSV mapping file with columns:\")\n",
    "print(\"- row_index: Position in the Hugging Face dataset\")\n",
    "print(\"- original_id: The '_id' field from the dataset\")\n",
    "print(\"- datastore_document_id: The ID returned by the datastore\")\n",
    "print(\"- title: Document title\")\n",
    "print(\"- text_preview: First 100 characters of text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head hf_document_mapping.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran about 58k documents (basically text chunks) so a total of 44MB in 895 minutes or 14.9 hours - around 60ish documents per minute. I had my settings to limit it to no more than 50 concurrent document uploads at one time. (If you need more let us know)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
