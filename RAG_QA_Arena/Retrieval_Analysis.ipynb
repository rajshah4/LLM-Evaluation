{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Analysis and Equivalence Evaluation\n",
    "\n",
    "This notebook analyzes the performance of a retrieval-augmented generation (RAG) agent on a set of QA tasks. It includes:\n",
    "- Loading and mapping gold document IDs to datastore document IDs\n",
    "- Running retrieval queries and collecting retrieved and attributed document IDs\n",
    "- Evaluating retrieval metrics (recall, precision, nDCG, etc.)\n",
    "- Using an LLM to evaluate the equivalence of agent responses to gold answers, with robust JSON enforcement and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import asyncio\n",
    "import csv\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datasets import load_dataset\n",
    "\n",
    "# RAG evaluation\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextRecall, NonLLMContextPrecisionWithReference\n",
    "\n",
    "# API clients\n",
    "from contextual import ContextualAI\n",
    "import anthropic\n",
    "\n",
    "# Data validation\n",
    "from pydantic import BaseModel, ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We load the annotated QA dataset and the mapping file that links gold document IDs to internal document IDs used by the retrieval system.\n",
    "\n",
    "This annotated data comes from the RAG QA Arena benchmark: From: https://github.com/awslabs/rag-qa-arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json('/Users/rajivshah/Code/rag-qa-arena/data/annotations_fiqa_with_citation.jsonl', lines=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rajistics/rag-qa-arena\")\n",
    "df = dataset['fiqa'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Gold Document IDs to Internal Document IDs\n",
    "\n",
    "To compare retrieval results with ground truth, we map the gold document IDs to the internal document IDs used by the the Contextual RAG datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('hf_document_mapping.csv')\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from original_id to datastore_document_id\n",
    "id_map = dict(zip(mapping['original_id'], mapping['datastore_document_id']))\n",
    "\n",
    "# Function to map a list of gold_doc_ids to their corresponding datastore_document_ids\n",
    "def map_doc_ids(gold_ids):\n",
    "    return [id_map.get(gid) for gid in gold_ids if gid in id_map]\n",
    "\n",
    "# Apply the function to the gold_doc_ids column\n",
    "df['datastore_document_ids'] = df['gold_doc_ids'].apply(map_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Retrieval Queries\n",
    "\n",
    "We define functions to query the retrieval system for each question, collecting both the retrieved document IDs and the document IDs actually cited in the agent's answer (attributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ContextualAI(api_key=\"key-\")\n",
    "agent_id = \"7a5f50ff-d205-4033-a7e5-529531230cb5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Function Overview: `run_query(user_input)`\n",
    "\n",
    "This function runs a retrieval-augmented generation (RAG) query against a custom agent and returns two levels of results:\n",
    "\n",
    "1. **`retrieved_chunks`** ‚Äì All retrieved chunks, including their document IDs and relevance scores.\n",
    "2. **`attribution_doc_ids`** ‚Äì A deduplicated list of document IDs that were actually cited in the model's final answer (i.e., the attributions), mapped from content-level IDs.\n",
    "3.  **`response`** ‚Äì The generated response from the model based on the retrievals.\n",
    "\n",
    "This helps distinguish between *what was retrieved* and *what was used* in the final response, enabling fine-grained evaluation of retrieval and grounding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(user_input):\n",
    "    \"\"\"\n",
    "    Run the retrieval agent for the given user_input.\n",
    "    Returns:\n",
    "        - retrieved_chunks: list of dicts with 'doc_id' and 'score'\n",
    "        - attribution_doc_ids: list of unique document IDs from attributions\n",
    "    \"\"\"\n",
    "    query_result = client.agents.query.create(\n",
    "        agent_id=agent_id,\n",
    "        messages=[{\n",
    "            \"content\": user_input,\n",
    "            \"role\": \"user\"\n",
    "        }],\n",
    "        include_retrieval_content_text=True,\n",
    "        retrievals_only=False\n",
    "    )\n",
    "\n",
    "    # Extract mapping from content_id to doc_id\n",
    "    content_to_doc = {\n",
    "        rc.content_id: rc.doc_id\n",
    "        for rc in query_result.retrieval_contents\n",
    "    }\n",
    "\n",
    "    # Extract retrieved_chunks\n",
    "    retrieved_chunks = [\n",
    "        {\"doc_id\": rc.doc_id}\n",
    "        for rc in query_result.retrieval_contents\n",
    "    ]\n",
    "\n",
    "    response = query_result.message.content\n",
    "\n",
    "    # Get document IDs referenced in attributions\n",
    "    attribution_doc_ids = set()\n",
    "    seen_attr = set()\n",
    "    for attr in query_result.attributions:\n",
    "        key = tuple(attr.content_ids)\n",
    "        if key not in seen_attr:\n",
    "            seen_attr.add(key)\n",
    "            for cid in attr.content_ids:\n",
    "                doc_id = content_to_doc.get(cid)\n",
    "                if doc_id:\n",
    "                    attribution_doc_ids.add(doc_id)\n",
    "\n",
    "    return retrieved_chunks, list(attribution_doc_ids), response\n",
    "\n",
    "def get_doc_ids(row):\n",
    "    retrieved, attributed, response = run_query(row['question'])\n",
    "    # Flatten retrieved to just doc_id strings\n",
    "    retrieved_ids = [r['doc_id'] for r in retrieved]\n",
    "    return pd.Series({\n",
    "        'retrievals': retrieved_ids,\n",
    "        'attributions': attributed,        \n",
    "        'response' : response,\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a test and make sure you are retrievals are aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (run_query(df['question'][1]))\n",
    "print(df['datastore_document_ids'][1])\n",
    "print(df['gold_doc_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your DataFrame\n",
    "df[['retrievals', 'attributions','response']] = df.head(10).apply(get_doc_ids, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample and Save Results\n",
    "\n",
    "To speed up evaluation, we sample 200 questions and run the retrieval and attribution pipeline, saving the results for further analysis.\n",
    "It can take about 40 minutes to query 200 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 200 rows reproducibly\n",
    "df_sample_200 = df.sample(n=200, random_state=42).copy()\n",
    "\n",
    "# Apply the function\n",
    "df_sample_200[['retrievals', 'attributions','response']] = df_sample_200.apply(get_doc_ids, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200.to_csv('fiqa_sample_200_with_retrievals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200 = pd.read_csv('fiqa_sample_200_with_retrievals.csv')\n",
    "df_sample_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence Evaluation with LLM Tool Call\n",
    "\n",
    "We use an LLM (Anthropic Claude) with a tool schema to robustly evaluate whether the agent's response is equivalent to the gold answer. The tool call enforces a strict JSON output, which is then validated with Pydantic for reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_client = anthropic.Anthropic(api_key=\"sk-ant-\") #update with your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Optional\n",
    "\n",
    "class EquivalenceResult(BaseModel):\n",
    "    score: Optional[float]\n",
    "    rationale: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalence_tool = {\n",
    "    \"name\": \"evaluate_equivalence\",\n",
    "    \"description\": \"Evaluate if the agent response is equivalent to the gold response.\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"score\": {\n",
    "                \"type\": \"number\",\n",
    "                \"enum\": [0.0, 1.0],\n",
    "                \"description\": \"1.0 if equivalent, 0.0 if not equivalent\"\n",
    "            },\n",
    "            \"rationale\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"One sentence justification for the score\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"score\", \"rationale\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalence_prompt = \"\"\"You are an expert that can identify similar responses. You will be given a set of questions, a gold response that correctly answers the questions and an agent response that is generated by a system. We are evaluating the gold response against the agent response.\n",
    "\n",
    "You are to compare the gold response with the agent response and rationalize whether the agent response is equivalent to the gold response.\n",
    "\n",
    "When testing equivalence, consider the following:\n",
    "- If the agent response is exactly the same as or very similar to the gold response, then they are equivalent.\n",
    "- If the agent response contains all of the information in the gold response and some additional information, then they are equivalent.\n",
    "- If the agent response contains an alternative good response that is grounded in the knowledge, then they are equivalent.\n",
    "- Do not bias towards verbose or succinct responses, the verbosity of agent response does not matter as long as it comprehensively includes all the information from the gold response.\n",
    "\n",
    "Given the question, gold response and agent response, come up with a step by step plan to analyse if the agent response is equivalent to the gold response. If you consider the agent response as a good alternative to the gold, you MUST justify your decision by citing the knowledge in your rationale. Think through this plan and end your output with a json.\n",
    "\"\"\"\n",
    "output_format = \"\"\"\n",
    "IMPORTANT: Format your evaluation as a JSON object with two keys:\n",
    "{\n",
    "    \"rationale\": \"[Your one sentence justification of why the responses are equivalent or not]\",\n",
    "    \"score\": [1.0 or 0.0]\n",
    "}\n",
    "\n",
    "Where 1.0 means \"equivalent\" and 0.0 means \"not equivalent\"\n",
    "\n",
    "IMPORTANT: Only output the JSON object, and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equivalence_score_and_rationale(question, gold, agent, prompt_template):\n",
    "    prompt = prompt_template + f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Gold response: {gold}\n",
    "\n",
    "Agent response: {agent}\n",
    "\"\"\" + output_format\n",
    "    \n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-latest\",\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        tools=[equivalence_tool],\n",
    "        tool_choice={\"type\": \"tool\", \"name\": \"evaluate_equivalence\"},\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    try:\n",
    "        if response.content[0].type == \"tool_use\":\n",
    "            result_dict = response.content[0].input\n",
    "            result = EquivalenceResult(**result_dict)\n",
    "            return result.score, result.rationale\n",
    "        else:\n",
    "            return None, f\"Unexpected response type: {response.content[0].type}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Error parsing response: {e}\\nRaw: {response.content[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Equivalence Evaluation\n",
    "\n",
    "We apply the equivalence evaluation to each row in our sample, storing both the equivalence score (1.0 for equivalent, 0.0 for not) and a rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_equivalence(row):\n",
    "    score, rationale = get_equivalence_score_and_rationale(\n",
    "        row['question'],\n",
    "        row['answer'],\n",
    "        row['response'],\n",
    "        equivalence_prompt\n",
    "    )\n",
    "    return pd.Series({'equivalence_score': score, 'equivalence_rationale': rationale})\n",
    "\n",
    "df_sample_200[['equivalence_score', 'equivalence_rationale']] = df_sample_200.apply(apply_equivalence, axis=1)\n",
    "df_sample_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200.to_csv('fiqa_sample_200_with_equivalence.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200 = pd.read_csv('fiqa_sample_200_with_equivalence.csv')\n",
    "df_sample_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Comparison Against LFQRA / Human Answer\n",
    "\n",
    "We use an LLM (Anthropic Claude) with to evaluate whether the agent's response is prefereable to the gold answer written by a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Comparison against human answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prompt = \"\"\"  system_prompt: |\n",
    "    We will show you a query and a pair of answers to the query. You need to provide your preference over this pair of answers.\n",
    "\n",
    "    First, try your best to determine whether the information in an answer can help truthfully answer the query. Then rate your preference based on Helpfulness and Truthfulness.\n",
    "    - Helpfulness: information that is helpful/relevant to answer the query. An ideal answer consists of only information that is helpful/relevant to answer the query.\n",
    "    - Truthfulness: information that you believe is correct to answer the query. By our definition, truthful information should be helpful information. If you find it difficult to determine the truthfulness of some information, consider it untruthful. Often time, this is due to not enough context provided in the answer. Another source of untruthfulness is when conflicting information presented, and the answer does not reconcile them in a coherent way.\n",
    "\n",
    "    <rubric>\n",
    "    Here is how you judge (in the order of importance),\n",
    "    - If one answer has all truthful information while the other has some untruthful information, prefer the all truthful one.\n",
    "    - If both have some untruthful information, prefer the one with less untruthful information.\n",
    "    - If both have all truthful information, prefer the one with more truthful or helpful information.\n",
    "    - If two answers look equally good, or it is too hard to judge using the 3 cases above, then you are our \"not sure\" which one is better.\n",
    "    </rubric>\n",
    "\n",
    "    ### Examples:\n",
    "\n",
    "    **Example 1**\n",
    "    User:\n",
    "    <query>\n",
    "    difference between publicly and publically.\n",
    "    </query>\n",
    "\n",
    "    <answer 1>\n",
    "    Both 'publicly' and 'publically' bear no difference in meaning, as they are essentially alternative spellings of the same concept. Publicly is more widely used, but the existence of 'publically' in reputable sources like the OED means it cannot be dismissed as simply incorrect. Some opinions hold that 'publicly' is the older irregular form, still preached by a lot of grammars, and 'publically,' on the other hand, is the newer and regular form.\n",
    "    </answer 1>\n",
    "\n",
    "    <answer 2>\n",
    "    There is no difference in meaning between 'publicly' and 'publically'; they are alternative spellings of the same word.\n",
    "    </answer 2>\n",
    "\n",
    "    Assistant:\n",
    "    <thinking>Both <answer 1> and <answer 2> are truthful. However, <answer 1> provides more truthful information as the context to compare the two terms. Therefore, <answer 1> is better.</thinking>\n",
    "    <rating>1</rating>\n",
    "\n",
    "    **Example 2**\n",
    "    User:\n",
    "    <query>\n",
    "    what did European/American historical cooks do with the egg whites?\n",
    "    </query>\n",
    "\n",
    "    <answer 1>\n",
    "    Historical European and American cooks used egg whites for making egg white omelettes and egg white pasta, as well as for stiffening clothing, similar to how starch is used today.\n",
    "    </answer 1>\n",
    "\n",
    "    <answer 2>\n",
    "    Egg whites have found their place in various non-culinary applications throughout history, such as in the clarification of beer and wine, in the conservation of books through bookbinding and gilding, and in makeup as an ancient form of nail polish. They were also utilized historically as a stiffening agent for clothing, similar to how starch is used today. The culinary landscape was not left untouched, with egg whites making their way into recipes for omelettes and pastas as early as the 15th century.\n",
    "    </answer 2>\n",
    "\n",
    "    Assistant:\n",
    "    <thinking>Both <answer 1> and <answer 2> provide several usages of egg whites for European/American cooks. <answer 2> clearly provides more options with fully explained details. Therefore, <answer 2> is better.</thinking>\n",
    "    <rating>2</rating>\n",
    "\n",
    "    **Example 3**\n",
    "    User:\n",
    "    <query>\n",
    "    should utf-16 be considered harmful?\n",
    "    </query>\n",
    "\n",
    "    <answer 1>\n",
    "    The question of whether UTF-16 should be considered harmful is subject to differing opinions. One perspective suggests that UTF-16 is harmful due to a historical misconception about character encoding and recommends UTF-8 as the superior choice for various text interfaces. Another viewpoint argues that UTF-16 is not inherently harmful, emphasizing its utility in specific scenarios where it serves as a compromise between simplicity and compactness. The choice between UTF-16 and other encodings like UTF-8 depends on the specific requirements of the application, such as compatibility with ASCII or the need to efficiently encode certain character sets.\n",
    "    </answer 1>\n",
    "\n",
    "    <answer 2>\n",
    "    UTF-16 should not be considered harmful. However, contrasting views argue that UTF-16 should indeed be considered harmful. Some argue that the very reason UTF-16 exists is because some time ago there used to be a misguided belief that WideChar is going to be what UCS-4 now is. Additionally, the harmfulness of UTF-16 is tied to issues with exercising code.\n",
    "    </answer 2>\n",
    "\n",
    "    Assistant:\n",
    "    <thinking>Both <answer 1> and <answer 2> reconcile the two conflicting views with detailed explanation. I am not sure which one is better.</thinking>\n",
    "    <rating>0</rating>\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "    Query is in the <query></query> tags. Answer 1 is in <answer 1></answer 1>, and Answer 2 is in <answer 2></answer 2>.\n",
    "\n",
    "    <query>\n",
    "    {question}\n",
    "    </query>\n",
    "\n",
    "    <answer 1>\n",
    "    {response1}\n",
    "    </answer 1>\n",
    "\n",
    "    <answer 2>\n",
    "    {response2}\n",
    "    </answer 2>\n",
    "\n",
    "    Review the rubric in <rubric> tags,\n",
    "    - if you prefer <answer 1>, output 1.\n",
    "    - if you prefer <answer 2>, output 2.\n",
    "    - if you are not sure, output 0.\n",
    "\n",
    "    First, think step by step, put your thinking in <thinking></thinking> tags. Your thinking must be shorter than 50 words. Then, provide your rating inside <rating></rating> tags. Remember your rating should be 0 if you are not sure, and your rating must be either 0, 1, or 2.\n",
    "\n",
    "    Return a single JSON object with two keys: `\"rating\"` containing your score and `\"evaluation\"` containing your thinking, but remove the tags.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preference_rating(question, response1, response2):\n",
    "    # Fill in the user prompt with the actual question and responses\n",
    "\n",
    "    prompt = comparison_prompt + f\"\"\"\n",
    "        Question: {question}\n",
    "\n",
    "        Answer 1: {response1}\n",
    "\n",
    "        Answer 2: {response2}\n",
    "        \"\"\" + user_prompt\n",
    "    \n",
    "    prompt = user_prompt.format(\n",
    "        question=question,\n",
    "        response1=response1,\n",
    "        response2=response2\n",
    "    )\n",
    "    # Compose the full prompt for the LLM\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    # Call the LLM (Anthropic, OpenAI, etc.)\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-latest\",\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        messages=messages\n",
    "    )\n",
    "    # Parse the output (should be a JSON object)\n",
    "    import json\n",
    "    try:\n",
    "        # If the model returns the JSON as a string, parse it\n",
    "        output = response.content[0].text if hasattr(response.content[0], \"text\") else response.content[0]\n",
    "        result = json.loads(output)\n",
    "        return result[\"rating\"], result[\"evaluation\"]\n",
    "    except Exception as e:\n",
    "        return None, f\"Error parsing response: {e}\\nRaw: {response.content[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preference(row):\n",
    "    rating, evaluation = get_preference_rating(\n",
    "        row['question'],\n",
    "        row['answer'],\n",
    "        row['response']\n",
    "    )\n",
    "    return pd.Series({'preference_rating': rating, 'preference_evaluation': evaluation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200[['preference_rating', 'preference_evaluation']] = df_sample_200.apply(apply_preference, axis=1)\n",
    "df_sample_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200.to_csv('fiqa_sample_200_with_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Retrieval Metrics\n",
    "\n",
    "We compute standard retrieval metrics (recall@k, precision@k, nDCG@k, etc.) for each question, using both retrieved and attributed document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(retrieved_ids, ground_truth_ids, k):\n",
    "    retrieved_top_k = set(retrieved_ids[:k])\n",
    "    return len(retrieved_top_k & ground_truth_ids) / len(ground_truth_ids) if ground_truth_ids else 0.0\n",
    "\n",
    "def precision_at_k(retrieved_ids, ground_truth_ids, k):\n",
    "    retrieved_top_k = retrieved_ids[:k]\n",
    "    return sum(1 for cid in retrieved_top_k if cid in ground_truth_ids) / k if k else 0.0\n",
    "\n",
    "def ndcg_at_k(retrieved_chunks, ground_truth_ids, k=10):\n",
    "    top_k = retrieved_chunks[:k]\n",
    "    relevances = [1 if chunk in ground_truth_ids else 0 for chunk in top_k]\n",
    "    dcg = sum((2**rel - 1) / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "    ideal_relevances = sorted([1]*min(len(ground_truth_ids), k) + [0]*(k - min(len(ground_truth_ids), k)), reverse=True)\n",
    "    idcg = sum((2**rel - 1) / np.log2(idx + 2) for idx, rel in enumerate(ideal_relevances))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def idcg(matched_chunk_positions):\n",
    "    total_relevant = len(matched_chunk_positions)\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return sum(1 / math.log2(i + 2) for i in range(total_relevant))\n",
    "\n",
    "def precision_at_r(retrieved_ids, ground_truth_ids):\n",
    "    r = len(ground_truth_ids)\n",
    "    retrieved_top_r = retrieved_ids[:r]\n",
    "    return sum(1 for cid in retrieved_top_r if cid in ground_truth_ids) / r if r else 0.0\n",
    "\n",
    "def hit_rate_at_k(retrieved_ids, ground_truth_ids, k):\n",
    "    return int(any(cid in ground_truth_ids for cid in retrieved_ids[:k]))\n",
    "\n",
    "def reciprocal_rank_at_k(retrieved_ids, ground_truth_ids, k):\n",
    "    for idx, cid in enumerate(retrieved_ids[:k]):\n",
    "        if cid in ground_truth_ids:\n",
    "            return 1.0 / (idx + 1)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def safe_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_query(row, k=10, use_attributions=False):\n",
    "    index_df = int(row['qid'])\n",
    "    user_input = row[\"question\"]\n",
    "    answer = row[\"answer\"]\n",
    "    response = row[\"response\"]\n",
    "    equivalence_rationale = row[\"equivalence_rationale\"]\n",
    "    preference_rationale = row[\"preference_evaluation\"]\n",
    "    ground_truth_ids = set(safe_list(row[\"datastore_document_ids\"]))\n",
    "    retriever_ids = safe_list(row[\"retrievals\"])\n",
    "\n",
    "    if use_attributions == True:\n",
    "        attributions_ids = row[\"attributions\"]\n",
    "        retrieved_ids = attributions_ids\n",
    "    else:\n",
    "        retrieved_ids = retriever_ids\n",
    "\n",
    "    if not ground_truth_ids:\n",
    "        # handle the empty case, e.g., skip, or set all metrics to 0 or np.nan\n",
    "        recall = precision = ndcg = icdg = precision_r = hit_rate = mrr = 0.0\n",
    "    else:\n",
    "        recall = recall_at_k(retrieved_ids, ground_truth_ids, k)\n",
    "        precision = precision_at_k(retrieved_ids, ground_truth_ids, k)\n",
    "        ndcg = ndcg_at_k(retrieved_ids, ground_truth_ids, k)\n",
    "        icdg = idcg(set(range(min(len(ground_truth_ids), k))))\n",
    "        precision_r = precision_at_r(retrieved_ids, ground_truth_ids)\n",
    "        hit_rate = hit_rate_at_k(retrieved_ids, ground_truth_ids, k)\n",
    "        mrr = reciprocal_rank_at_k(retrieved_ids, ground_truth_ids, k)\n",
    "\n",
    "    # --- RAGAS metrics ---\n",
    "    reference_contexts = list(ground_truth_ids)\n",
    "    retrieved_contexts = retrieved_ids\n",
    "\n",
    "    if not reference_contexts or not retrieved_contexts:\n",
    "        recall_score, precision_score = 0.0, 0.0  # or np.nan, or skip this row\n",
    "    else:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=user_input,\n",
    "            reference_contexts=reference_contexts,\n",
    "            retrieved_contexts=retrieved_contexts,\n",
    "        )\n",
    "        async def run_ragas_metrics(sample):\n",
    "            context_recall = NonLLMContextRecall()\n",
    "            context_precision = NonLLMContextPrecisionWithReference()\n",
    "            recall_score = await context_recall.single_turn_ascore(sample)\n",
    "            precision_score = await context_precision.single_turn_ascore(sample)\n",
    "            return recall_score, precision_score\n",
    "        recall_score, precision_score = asyncio.run(run_ragas_metrics(sample))\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"index\": index_df,\n",
    "        \"user_input\": user_input,\n",
    "        \"answer\": answer,\n",
    "        \"response\": response,\n",
    "        \"equivalence_rationale\": equivalence_rationale,\n",
    "        \"preference_rationale\": preference_rationale,\n",
    "        \"k\": k,\n",
    "        \"recall@k\": recall,\n",
    "        \"precision@k\": precision,\n",
    "        \"precision@R\": precision_r,\n",
    "        \"nDCG@k\": float(ndcg),\n",
    "        \"iDCG@k\": float(icdg),\n",
    "        \"hit_rate@k\": hit_rate,\n",
    "        \"mrr@k\": mrr,\n",
    "        \"ragas_context_recall\": recall_score,\n",
    "        \"ragas_context_precision\": precision_score,\n",
    "        \"ground_truth_ids\": list(ground_truth_ids),\n",
    "        \"retriever_ids\": retriever_ids,\n",
    "       # \"attributions\": attributions_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and Summarize Results\n",
    "\n",
    "We aggregate the results across all questions to report mean retrieval and equivalence metrics, providing an overall view of system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first example (row 0) from your DataFrame\n",
    "row = df_sample_200.iloc[0]\n",
    "\n",
    "# Run the evaluation for this single example\n",
    "result = evaluate_single_query(row, k=10,use_attributions=False)\n",
    "\n",
    "# Print the results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, row in df_sample_200.iterrows():\n",
    "    try:\n",
    "       # print(row)\n",
    "        result = evaluate_single_query(row, k=15,use_attributions=False)\n",
    "        score = row.get('equivalence_score', None)\n",
    "        preference_score = row.get('preference_rating', None)\n",
    "        try:\n",
    "            score = int(float(score)) if score is not None else None\n",
    "        except Exception:\n",
    "            score = None\n",
    "        result['equivalence_score'] = score\n",
    "        result['preference_rating'] = preference_score\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on row {idx}: {e}\")\n",
    "        results.append({\n",
    "          \"ragas_context_recall\": 0.0,\n",
    "          \"ragas_context_precision\": 0.0,\n",
    "      })\n",
    "        # Optionally, append a placeholder or skip\n",
    "        continue\n",
    "\n",
    "metrics = [\n",
    "    'recall@k',\n",
    "    'precision@k',\n",
    "    'precision@R',\n",
    "    'nDCG@k',\n",
    "    'iDCG@k',\n",
    "    'ragas_context_recall',\n",
    "    'ragas_context_precision',\n",
    "    'equivalence_score',\n",
    "    'preference_rating',\n",
    "]\n",
    "\n",
    "# Final save\n",
    "final = pd.DataFrame(results)\n",
    "final.to_csv(\"retrieval_eval_results_final.csv\", index=False)\n",
    "\n",
    "means = final[metrics].mean()\n",
    "print(means)\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a robust, end-to-end evaluation of a RAG system, including both retrieval quality and answer equivalence, with enforced structured outputs for reliability and reproducibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
